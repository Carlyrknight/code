---
title: "Data Descriptives, Part 1"
author: "Carly Knight"
date: "3/2/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringr)
knitr::opts_chunk$set(echo = TRUE)
```

# Prep Data
```{r r1, echo=FALSE}
setwd("~/Documents/Data/Annual Report/report_paragraphs/future_texts/")

#create text tibble
worker_list_of_files <- list.files(".", pattern = "*.txt")

#create text tibble
list_of_files <- list.files(pattern = "*.txt")

tbl <- list_of_files %>% 
  map_chr(~ read_file(.)) %>% 
  tibble(text = .)

tbl$filename<-list_of_files

#read in metadata
metadata<- read.csv(file="metadata_futureperfect.csv", quote="", stringsAsFactors=FALSE) %>%
  dplyr::rename(filename=Filename)
nrow(metadata) 
#39,883

#define narrow naics and broad naics
metadata <- metadata %>%
  mutate(NAIC_2   = substr(NAIC, 1, 2)) %>%
  mutate(NAIC_cat= recode(NAIC_2,`11`= "Agriculture",
                          `21` = "Mining and Extraction",
                          `22` = "Utilities", 
                          `23` = "Construction",
                          `31` ="Manufacturing", 
                          `32`= "Manufacturing", 
                          `33` = "Manufacturing", 
                          `42`= "Wholesale", 
                          `44`= "Retail",
                          `45`= "Retail",
                          `48`= "Transportation and Warehousing", 
                          `49`= "Transportation and Warehousing", 
                          `51`= "Information", 
                          `52` = "Finance and Insurance", 
                          `53`= "Real Estate and Rental Leasing", 
                          `54`= "Professional Scientific and Technical Services", 
                          `55`= "Management", 
                          `56`= "Administrative support and Waste management",
                          `62`= "Healthcare and Social" , 
                          `71`= "Arts Entertainment and Recreation", 
                          `72` = "Accomodation and Food service", 
                          `81`= "Other Services"))

metadata <- metadata %>%
  mutate(NAIC_broad = recode(NAIC_cat, "Transportation and Warehousing"  = "Wholesale and shipping",
                             "Wholesale" = "Wholesale and shipping",
                             "Finance and Insurance" = "FIRE",
                             "Real Estate and Rental Leasing" = "FIRE",
                             "Manufacturing" ="Manufacturing",
                             "Information" = "Info. and professional services",
                             "Professional Scientific and Technical Services" = "Info. and professional services",
                             "Administrative support and Waste management" = "Info. and professional services",
                             "Management" = "Info. and professional services",
                             "Retail" = "Retail, social and other" ,
                              "Accomodation and Food service"= "Retail, social and other" ,
                             "Other Services"= "Retail, social and other" ,
                             "Healthcare and Social" = "Retail, social and other" ,
                             "Arts Entertainment and Recreation"= "Retail, social and other" ,
                             "Agriculture"  = "Primary, utilities and construction",
                             "Mining and Extraction" = "Primary, utilities and construction",
                              "Utilities" = "Primary, utilities and construction",
                             "Construction"= "Primary, utilities and construction"))
                             
metadata<- metadata %>%
  group_by(Year)%>%
  mutate(quantile_rank = ntile(AnrAssetDispIdxNum,4))
                             
                  
#merge metadata + txt files
tbl$filename<-gsub(".txt", "", tbl$filename)
metadata$filename<-gsub(".xml", "", metadata$filename)
tbl <- left_join(tbl, metadata)
nrow(tbl)
#40,933
```

# Basics Descriptives: Document Level
```{r basic}
floor_decade    = function(value){ return(value - value %% 10) }

data<-tbl%>%
  filter(Year>=1930 & Year <2005)%>%
  mutate(decade = floor_decade(Year))%>%
  mutate(numwords_r = str_count(text))

summary<-data %>%
  mutate(paragraphs_dropped = num_paragraphs_prespellcheck - num_paragraphs_postspellcheck)%>%
  group_by(Year)%>%
  summarize(mean_numwords = mean(numwords_r, na.rm=T), 
            N = n())
summary %>%
  ggplot(aes(x=Year, y = mean_numwords)) +
  geom_line()+
  ggtitle("Number of words over time")

summary %>%
  ggplot(aes(x=Year, y = N)) +
  geom_line()+
  ggtitle("Number of docs over time")
```
# Basics Descriptives: Future-Words
```{r basic1}
#create my own stop words
mystopwords <- tibble(word = c("co", "a", "cco", "the", "thc", "sk", "dca", "cg", "pap", "po", "txu", "corco", "cobo", "ourselves", "hers", "between", "yourself", "but", "again", "there", "about", "once", "during", "out", "very", "having", "with", "they", "own", "an", "be", "some", "for", "do", "its", "yours", "such", "into", "of", "most", "itself", "other", "off", "is", "s", "am", "or", "who", "as", "from", "him", "each", "the", "themselves", "until", "below", "are", "we", "these", "your", "his", "through", "don", "nor", "me", "were", "her", "more", "himself", "this", "down", "should", "our", "their", "while", "above", "both", "up", "to", "ours", "had", "she", "all", "no", "when", "at", "any", "before", "them", "same", "and", "been", "have", "in", "will", "on", "does", "yourselves", "then", "that", "because", "what", "over", "why", "so", "can", "did", "not",  "under", "he", "you", "herself", "has", "just", "where", "too", "only", "myself", "which", "those", "i", "after", "few", "whom", "t", "being", "if", "theirs", "my", "against", "a", "by", "doing", "it", "how", "further", "was", "here", "than", "but", "oh", "or", "on"))
         
#clean: words
data1<-data %>%
  mutate(text = gsub("\\*", "", text)) %>%
  mutate(text = tolower(text)) %>%
  mutate(text = gsub("[[:digit:]]", "", text))%>%
  mutate(text = gsub("[[:punct:]]", "", text))

#tokenize 
tbl_tokens <- data1 %>%
  unnest_tokens(word, text) %>%
  mutate(ncharacters = nchar(word))%>%
  anti_join(mystopwords)

low_freq <- tbl_tokens %>%
  count(word, sort = TRUE)%>%
  filter(n <=30)

short_word<- tbl_tokens %>%
  filter(ncharacters <=2)

low_freq_words<- tibble(word = low_freq$word)
short_words <- tibble(word = short_word$word)

#remove low-frequency misspelings & 2 characters or less
tbl_tokens<- tbl_tokens %>%
  anti_join(low_freq_words) %>%
  anti_join(short_words)

#group by decade, future words
v <- c(  'may', 'might', 'future', 'will','optimism', 'pessimism', 'uncertainty', 'certainty',  'outlook', 'risk', 'risky', 'optimistic')

decade_words<- tbl_tokens%>%
  group_by(decade)%>%
  count(word, sort = TRUE) 

decade_totals <- decade_words %>%
  group_by(decade)%>%
  summarize(total= sum(n))

decade_words<- left_join(decade_words, decade_totals)
  
decade_words%>%
  filter(word %in% v) %>%
  mutate(percent = n/total) %>%
  ggplot(aes(decade, percent))+
  geom_line()+
  facet_wrap(~word, scales = "free")+
  ggtitle("Future words in text")

#tf-idf
decade_words_idf <- decade_words %>%
  bind_tf_idf(word, decade, n)

decade_words_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(decade) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = as.factor(decade))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~decade, ncol = 4, scales = "free") +
  coord_flip()+
  ggtitle("TF-IDF By Decade")

rm(data, low_freq)
gc()

```

# Dictionary words
```{r dictionary, echo =FALSE}
library(readxl)
#Concreteness ratings
#http://crr.ugent.be/archives/1330
concrete <- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/Concreteness_ratings_Brysbaert_et_al_BRM.csv")
concrete<-concrete %>%
  mutate(word = tolower(Word))

#Positive/Negative Snetiment
neg_emo <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Negative",col_names = "word")
constraint <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Constraining",col_names = "word")
pos_emo<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Positive",col_names = "word")
uncertainty<- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/uncertainty_businss_loughran_mcdonald.csv")
weak_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "WeakModal",col_names = "word")
strong_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "StrongModal",col_names = "word")
future_words <-read.csv(file = "/Users/carlyknight/Documents/Data/dictionaries/liwc2015dict.csv")   
future_words<- as_tibble(future_words)%>%
  filter(var == "FocusFuture")%>%
  mutate(word = gsub("\\*", "", word))%>%
  select(-c(var))

#lowercase dictionaries
neg_emo<- neg_emo %>% mutate(word = tolower(word)) %>% mutate(negsent = 1)
pos_emo<- pos_emo %>% mutate(word = tolower(word)) %>% mutate(possent = 1)
strong_modal<- strong_modal %>% mutate(word = tolower(word)) %>% mutate(strongmod = 1)
weak_modal<- weak_modal %>% mutate(word = tolower(word)) %>% mutate(weakmod = 1)
uncertainty<- uncertainty %>% mutate(word = tolower(word)) %>% mutate(uncertainty = 1)
constraint<- constraint %>% mutate(word = tolower(word)) %>% mutate(constraint = 1)
futurewords<- future_words %>% mutate(word = tolower(word)) %>% mutate(future = 1)


#turn into a matrix
dic_matrix <- full_join(neg_emo, pos_emo)
dic_matrix <- full_join(dic_matrix, strong_modal)
dic_matrix <- full_join(dic_matrix, weak_modal)
dic_matrix <- full_join(dic_matrix, uncertainty)
dic_matrix <- full_join(dic_matrix, constraint)
dic_matrix <- full_join(dic_matrix, futurewords)

dic_matrix<- dic_matrix %>%
  replace(is.na(.), 0)

#join
tbl_tokens1 <- left_join(tbl_tokens, dic_matrix)
#save(tbl_tokens1, file = "/Users/carlyknight/Dropbox/PROJECTS/ConceptionsofRisk/data/dtm/tbl_tokens1.R" )

#year
year_words<- tbl_tokens1%>%
  mutate(possent = ifelse(is.na(possent), 0, possent)) %>%
  mutate(negsent = ifelse(is.na(negsent), 0, negsent)) %>%
  mutate(uncertainty = ifelse(is.na(uncertainty), 0, uncertainty)) %>%
  mutate(constraint = ifelse(is.na(constraint), 0, constraint)) %>%
  mutate(strongmod = ifelse(is.na(strongmod), 0, strongmod)) %>%
  mutate(weakmod = ifelse(is.na(weakmod), 0, weakmod)) %>%
  group_by(Year)%>%
  summarize(mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T),
            mean_constraint = mean(constraint, na.rm =T),
            mean_weakmod = mean(weakmod, na.rm=T),
            mean_strongmod = mean(strongmod, na.rm =T),
            N = n())

year_words%>%
  gather(key = varname, value = value, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time")
ggsave("wordscoresovertime2.pdf")

year_words%>%
  gather(key = varname, value = value,mean_constraint, mean_weakmod, mean_strongmod)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time")

year_words%>%
  gather(key = varname, value = value, mean_concrete)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
   ylim(1, 3.25)+
  facet_wrap(~varname, scales = "free", ncol =2)+
  ggtitle("Word Scores over time")
ggsave("concreteovertime.pdf")

#by industry
tbl_tokens1%>%
  mutate(possent = ifelse(is.na(possent), 0, possent)) %>%
  mutate(negsent = ifelse(is.na(negsent), 0, negsent)) %>%
  mutate(uncertainty = ifelse(is.na(uncertainty), 0, uncertainty)) %>%
  mutate(constraint = ifelse(is.na(constraint), 0, constraint)) %>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n()) %>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive,  mean_uncertainty)%>%
  ggplot(aes(Year, value, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, industry")

#by industry
tbl_tokens1%>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_concrete = mean(Conc.M, na.rm=T),
            N = n()) %>%
  gather(key = varname, value = value, mean_concrete)%>%
  ggplot(aes(Year, value, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Concrete words Scores over time, industry")


tbl_tokens1%>%
  mutate(possent = ifelse(is.na(possent), 0, possent)) %>%
  mutate(negsent = ifelse(is.na(negsent), 0, negsent)) %>%
  mutate(uncertainty = ifelse(is.na(uncertainty), 0, negsent)) %>%
  mutate(constraint = ifelse(is.na(constraint), 0, constraint)) %>%
  group_by(Year, quantile_rank)%>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
              mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n())%>%
  filter(quantile_rank ==1 | quantile_rank ==4)%>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, group = as.factor(quantile_rank), color = as.factor(quantile_rank)))+
  geom_point()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, size")

```

# Words changing over time
```{r models, echo=FALSE}
#total number words
nwords<-  length(unique(tbl_tokens1$word))

graph_word_change<- function(wordvar){

#calculate word frequency
words_by_time <- tbl_tokens1 %>%
  filter(get(wordvar) ==1)%>%
  count(Year, word)%>%
  group_by(Year) %>%
  mutate(words_year_total = sum(n)) %>%
  group_by(word)%>%
  mutate(words_dataset_total = sum(n))%>%
  group_by(Year, word) %>%
  mutate(word_total = sum(n)) %>%
  mutate(word_freq = words_dataset_total/nwords)%>%
  mutate(word_percent =word_total/words_year_total )%>%
  ungroup() %>%
  filter(word_total > 30)%>%
  select(Year, word,word_percent, word_freq)
 
models1 <- words_by_time %>%
  group_by(word) %>%
  do(tidy(lm( word_percent~ Year, .,))) %>%
  ungroup() %>%
  filter(term =="Year")%>%
  mutate(adjustedp = p.adjust(p.value))

freqs<- unique(words_by_time %>% 
                 select(word, word_freq))

top.slopes<- models1 %>% 
  filter(adjustedp <.05)

top.slopes%>%
  inner_join(freqs) %>%
  ggplot(aes(estimate, word_freq)) +
  geom_point(size = 0.5) +
  scale_y_log10() +
  geom_text(aes(label = word), check_overlap = TRUE)+
  xlab("Estimated change over time") +
  ylab("Frequency, logged scale")+
  geom_vline(xintercept=0, color="red")+
  theme_minimal()+
  ggtitle(paste(wordvar, ": word change over time", sep = " "))
}

#Graph
word_types<- c("possent", "negsent", "uncertainty", "constraint", "weakmod", "future")

for (i in word_types){
  
graph_word_change(i)
  
}



```

# READABILITY 
```{r readability, echo = FALSE}
library(quanteda)

#create corpus
corpus <- corpus(data, docid_field = "filename", text_field = "text")

#readability
read<-textstat_readability(corpus, measure = c("Flesch", "FOG", "meanSentenceLength", "meanWordSyllables"), remove_hyphens = TRUE, min_sentence_length = 1, max_sentence_length = 10000)

#merge with dates
read2<-read %>%
  rename(filename = document)%>%
  left_join(data)%>%
  group_by(Year)%>%
  summarize(mean_fog = mean(FOG, na.rm =T),
            mean_Flesch = mean(Flesch, na.rm =T),
            mean_sentencelength = mean(meanSentenceLength, na.rm =T),
            mean_syllables = mean(meanWordSyllables, na.rm =T),
            N = n())

read2%>%
  gather(key = varname, value = value, mean_fog, mean_Flesch, mean_sentencelength, mean_syllables)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  ggtitle("Readability score over time")+
  theme_minimal()
ggsave("readability.pdf")

```