---
title: "Data Descriptives, Analyze Data (Part 2)"
author: "Carly Knight"
date: "5/14/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringr)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())
load("/Users/carlyknight/Dropbox/PROJECTS/ConceptionsofRisk/data/dtm/text_data_prep.R")

#data_text (data1): numbers removed
#data_full (data): numbers remain
#format(object.size(tbl_tokens), units = "Gb") #8.2Gb
```

# Basics Descriptives: Document Level
```{r basic}
summary<-data_text %>%
  mutate(paragraphs_dropped = num_paragraphs_prespellcheck - num_paragraphs_postspellcheck)%>%
  group_by(Year)%>%
  summarize(mean_numwords = mean(numwords_r, na.rm=T), 
            N = n())
summary %>%
  ggplot(aes(x=Year, y = mean_numwords)) +
  geom_line()+
  ggtitle("Number of words over time")

summary %>%
  ggplot(aes(x=Year, y = N)) +
  geom_line()+
  ggtitle("Number of docs over time")
```

# Basics Descriptives: Future-Words
```{r basic1}
#group by decade, future words
v <- c(  'may', 'might', 'future', 'will','optimism', 'pessimism', 'uncertainty', 'certainty',  'outlook', 'risk', 'risky', 'optimistic')

decade_words<- tbl_tokens%>%
  group_by(decade)%>%
  count(word, sort = TRUE) 

decade_totals <- decade_words %>%
  group_by(decade)%>%
  summarize(total= sum(n))

decade_words<- left_join(decade_words, decade_totals)
  
decade_words%>%
  filter(word %in% v) %>%
  mutate(percent = n/total) %>%
  ggplot(aes(decade, percent))+
  geom_line()+
  facet_wrap(~word, scales = "free")+
  ggtitle("Future words in text")

#tf-idf
decade_words_idf <- decade_words %>%
  bind_tf_idf(word, decade, n)

decade_words_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(decade) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = as.factor(decade))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~decade, ncol = 4, scales = "free") +
  coord_flip()+
  ggtitle("TF-IDF By Decade")

```


# Dictionary words
```{r dictionary, echo =FALSE}
#Concreteness ratings
#http://crr.ugent.be/archives/1330
concrete <- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/Concreteness_ratings_Brysbaert_et_al_BRM.csv")

concrete<-concrete %>%
  mutate(word = tolower(Word))

#Positive/Negative Snetiment
neg_emo <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Negative",col_names = "word")
constraint <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Constraining",col_names = "word")
pos_emo<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Positive",col_names = "word")
uncertainty<- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/uncertainty_businss_loughran_mcdonald.csv")
weak_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "WeakModal",col_names = "word")
strong_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "StrongModal",col_names = "word")

#concrete
concrete <- concrete %>%select(word, Conc.M)
  
#future
future_words <-read.csv(file = "/Users/carlyknight/Documents/Data/dictionaries/liwc2015dict.csv")   
future_words<- as_tibble(future_words)%>%
  filter(var == "FocusFuture")%>%
  mutate(word = gsub("\\*", "", word))%>%
  select(-c(var))

#lowercase dictionaries
neg_emo<- neg_emo %>% mutate(word = tolower(word)) %>% mutate(negsent = 1)
pos_emo<- pos_emo %>% mutate(word = tolower(word)) %>% mutate(possent = 1)
strong_modal<- strong_modal %>% mutate(word = tolower(word)) %>% mutate(strongmod = 1)
weak_modal<- weak_modal %>% mutate(word = tolower(word)) %>% mutate(weakmod = 1)
uncertainty<- uncertainty %>% mutate(word = tolower(word)) %>% mutate(uncertainty = 1)
constraint<- constraint %>% mutate(word = tolower(word)) %>% mutate(constraint = 1)
futurewords<- future_words %>% mutate(word = tolower(word)) %>% mutate(future = 1)
concrete<- concrete %>% mutate(word = tolower(word)) %>% mutate(concrete = Conc.M) %>% select(word, concrete)

#turn into a matrix
dic_matrix <- full_join(neg_emo, pos_emo)
dic_matrix <- full_join(dic_matrix, strong_modal)
dic_matrix <- full_join(dic_matrix, weak_modal)
dic_matrix <- full_join(dic_matrix, uncertainty)
dic_matrix <- full_join(dic_matrix, constraint)
dic_matrix <- full_join(dic_matrix, futurewords)
dic_matrix <- full_join(dic_matrix, concrete)


#join
tbl_tokens1 <- left_join(tbl_tokens, dic_matrix)

tbl_tokens1 <- tbl_tokens1 %>%
  select(filename, Year, NAIC_broad, quantile_rank, word, negsent:concrete)%>%
  mutate(possent = ifelse(is.na(possent), 0, possent)) %>%
  mutate(negsent = ifelse(is.na(negsent), 0, negsent)) %>%
  mutate(uncertainty = ifelse(is.na(uncertainty), 0, uncertainty)) %>%
  mutate(constraint = ifelse(is.na(constraint), 0, constraint)) %>%
  mutate(strongmod = ifelse(is.na(strongmod), 0, strongmod)) %>%
  mutate(future = ifelse(is.na(future), 0, future)) %>%
  mutate(weakmod = ifelse(is.na(weakmod), 0, weakmod))

rm(tbl_tokens)
gc()
#save(tbl_tokens1, file = "/Users/carlyknight/Dropbox/PROJECTS/ConceptionsofRisk/data/dtm/tbl_tokens1.R" )
#format(object.size(tbl_tokens1), units = "Gb") #11.4Gb

#year
year_words<- tbl_tokens1%>%
  group_by(Year)%>%
  summarize(mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T),
            mean_constraint = mean(constraint, na.rm =T),
            mean_weakmod = mean(weakmod, na.rm=T),
            mean_strongmod = mean(strongmod, na.rm =T),
            mean_future = mean(future, na.rm =T),
            mean_concrete = mean(concrete, na.rm=T),
            N = n())

year_words%>%
  gather(key = varname, value = value, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time 1 ")

year_words%>%
  gather(key = varname, value = value, mean_constraint, mean_weakmod, mean_strongmod)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time 2")

year_words%>%
  gather(key = varname, value = value, mean_concrete)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  ylim(2, 3.5)+
  ggtitle("Concrete over time") 

year_words%>%
  gather(key = varname, value = value, mean_future)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_line()+
  xlim(1930, 2005)+
  ylim(0, .02)+
  ggtitle("Future Words over time") 
```

### Dictionary Words, by Subgroup
```{r dictionarysub, echo =FALSE}
#by industry
tbl_tokens1%>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n()) %>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive,  mean_uncertainty)%>%
  ggplot(aes(Year, value, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, industry")

tbl_tokens1%>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_concrete = mean(concrete, na.rm =T)) %>%
  ggplot(aes(Year, mean_concrete, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  ylim(1.5, 3.5)+
  ggtitle("Concrete Scores over time, industry")

tbl_tokens1%>%
  group_by(Year, quantile_rank)%>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
              mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n())%>%
  filter(quantile_rank ==1 | quantile_rank ==4)%>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, group = as.factor(quantile_rank), color = as.factor(quantile_rank)))+
  geom_point()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, size")

```

# Words changing over time
```{r models, echo=FALSE}
#total number words
nwords<-  length(unique(tbl_tokens1$word))

graph_word_change<- function(wordvar){

#calculate word frequency
words_by_time <- tbl_tokens1 %>%
  filter(get(wordvar) ==1)%>%
  count(Year, word)%>%
  group_by(Year) %>%
  mutate(words_year_total = sum(n)) %>%
  group_by(word)%>%
  mutate(words_dataset_total = sum(n))%>%
  group_by(Year, word) %>%
  mutate(word_total = sum(n)) %>%
  mutate(word_freq = words_dataset_total/nwords)%>%
  mutate(word_percent =word_total/words_year_total )%>%
  ungroup() %>%
  filter(word_total > 30)%>%
  select(Year, word,word_percent, word_freq)
 
models1 <- words_by_time %>%
  group_by(word) %>%
  do(tidy(lm( word_percent~ Year, .,))) %>%
  ungroup() %>%
  filter(term =="Year")%>%
  mutate(adjustedp = p.adjust(p.value))

freqs<- unique(words_by_time %>% 
                 select(word, word_freq))

top.slopes<- models1 %>% 
  filter(adjustedp <.05)

top.slopes%>%
  inner_join(freqs) %>%
  ggplot(aes(estimate, word_freq)) +
  geom_point(size = 0.5) +
  scale_y_log10() +
  geom_text(aes(label = word), check_overlap = TRUE)+
  xlab("Estimated change over time") +
  ylab("Frequency, logged scale")+
  geom_vline(xintercept=0, color="red")+
  theme_minimal()+
  ggtitle(paste(wordvar, ": word change over time", sep = " "))
}

#Graph
word_types<- c("possent", "negsent", "uncertainty", "constraint", "weakmod", "future")


graph_word_change("possent")
graph_word_change("negsent")
graph_word_change("constraint")
graph_word_change("future")
graph_word_change("uncertainty")
graph_word_change("weakmod")
graph_word_change("strongmod")
```

# Readability 
```{r readability, echo = FALSE}
library(quanteda)

#create corpus
# corpus <- corpus(data_full, docid_field = "filename", text_field = "text")
# 
# #readability
# read<-textstat_readability(corpus, measure = c("Flesch", "FOG", "meanSentenceLength", "meanWordSyllables"), remove_hyphens = TRUE, min_sentence_length = 1, max_sentence_length = 10000)
# 
# #merge with dates
# read2<-read %>%
#   rename(filename = document)%>%
#   left_join(data)%>%
#   group_by(Year)%>%
#   summarize(mean_fog = mean(FOG, na.rm =T),
#             mean_Flesch = mean(Flesch, na.rm =T),
#             mean_sentencelength = mean(meanSentenceLength, na.rm =T),
#             mean_syllables = mean(meanWordSyllables, na.rm =T),
#             N = n())
# 
# read2%>%
#   gather(key = varname, value = value, mean_fog, mean_Flesch, mean_sentencelength, mean_syllables)%>%
#   ggplot(aes(Year, value, color = varname))+
#   geom_point()+
#   xlim(1930, 2005)+
#   ggtitle("Readability score over time")+
#   theme_minimal()
# ggsave("readability.pdf")

```

#Number words
```{r numbers, echo=FALSE}
#generate graphs
number_means<- numbers %>%
  group_by(Year) %>%
  summarize(mean_per_numbers = mean(percent_numbers, na.rm=T))

number_means%>%
  ggplot(aes(Year, mean_per_numbers))+
  geom_point()+
  ylim(0, .07)+
  xlim(1930, 2005)+
  ggtitle("Number words over time")
```

# STM Topics

### Main Topics: From 5 Topic Solution
```{r stm1, echo=FALSE}
#summarize
summary<-data_full %>%
  group_by(Year)%>%
  summarize(t0 = mean(t0, na.rm=T),
            t1 = mean(t1, na.rm=T), 
            t2 = mean(t2, na.rm=T),
            t3 = mean(t3, na.rm=T),
            t4 = mean(t4, na.rm=T))

#add keywords
summary_long<-summary %>%
  gather(key = topic, value = value, t0:t4) 

summary_long <- summary_long %>%
  left_join(dtm_parquet_topics)%>%
  mutate(topic_name = paste(topic, keywords, sep = ": "))

#plot
ggplot(summary_long, aes(x=Year, y = value, group = topic_name, color = topic_name)) +
  geom_line()+
  ggtitle("Topics over time, dichotomized: median split")
```

### Main Topics: From 5 Topic Solution, by Group
```{r stm1sub, echo=FALSE}

#BY NAIC

#summarize
summary<-data_full %>%
  group_by(Year, NAIC_broad)%>%
  summarize(t0 = mean(t0, na.rm=T),
            t1 = mean(t1, na.rm=T), 
            t2 = mean(t2, na.rm=T),
            t3 = mean(t3, na.rm=T),
            t4 = mean(t4, na.rm=T))

#add keywords
summary_long<-summary %>%
  gather(key = topic, value = value, t0:t4) 

summary_long <- summary_long %>%
  left_join(dtm_parquet_topics)%>%
  mutate(topic_name = paste(topic, keywords, sep = ": "))%>%
  filter(!is.na(NAIC_broad))

#plot
ggplot(summary_long, aes(x=Year, y = value, group = NAIC_broad, color = NAIC_broad)) +
  geom_line()+
  facet_wrap(~topic)+
  ggtitle("Topics over time, by NAIC")

#BY SIZE

#summarize
summary<-data_full %>%
  group_by(Year, quantile_rank)%>%
  summarize(t0 = mean(t0, na.rm=T),
            t1 = mean(t1, na.rm=T), 
            t2 = mean(t2, na.rm=T),
            t3 = mean(t3, na.rm=T),
            t4 = mean(t4, na.rm=T))

#add keywords
summary_long<-summary %>%
  gather(key = topic, value = value, t0:t4) 

summary_long <- summary_long %>%
  left_join(dtm_parquet_topics)%>%
  mutate(topic_name = paste(topic, keywords, sep = ": "))%>%
  mutate(quantile_rank = as.factor(quantile_rank))

#plot
ggplot(summary_long, aes(x=Year, y = value, group = quantile_rank, color = quantile_rank)) +
  geom_line()+
  facet_wrap(~topic)+
  ggtitle("Topics over time, by Size")


```

#Lexicon analysis by topics, MEAN SPLIT SCORES
```{r stm2, echo=FALSE}

#dichotomize topics
topics<-unique(dtm_parquet_topics$topic)
years<- sort(unique(data_full$Year))

#median overall
for (i in topics){
  
  #discretize
   mean.varname <- paste(i, "mean.d" , sep=".")
   median.varname <- paste(i, "median.d" , sep=".")
   
   data_full<-data_full %>%
     mutate(mean.d= mean(!!as.name(i), na.rm =T ), median.d = median(!!as.name(i), na.rm =T))%>%
     mutate(!!mean.varname := ifelse(!!as.name(i) >= mean.d, 1, 0))%>%
     mutate(!!median.varname := ifelse(!!as.name(i) >= median.d, 1, 0))
}

#merge topics into tokens
topic_df<- data_full %>%
  select(filename, t0.mean.d, t1.mean.d, t2.mean.d, t3.mean.d, t4.mean.d)

tbl_tokens_topics<- tbl_tokens1 %>%
  left_join(topic_df)
#format(object.size(tbl_tokens_topics), units = "Gb")
rm(tbl_tokens1)
gc()

###############################
#Uncertainty by topic: By Mean
###############################

#gather by topic, select topics
tbl_tokens_topics_long<-tbl_tokens_topics%>%
  gather(key = topic, value = value, t0.mean.d:t4.mean.d) %>%
  filter(value ==1)

topic_words<- tbl_tokens_topics_long%>%
  group_by(Year, topic)%>%
  summarize(mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T),
            mean_constraint = mean(constraint, na.rm =T),
            mean_weakmod = mean(weakmod, na.rm=T),
            mean_strongmod = mean(strongmod, na.rm =T),
            mean_future = mean(future, na.rm =T),
            mean_concrete = mean(concrete, na.rm =T),
            N = n())

ggplot(topic_words, aes(Year, mean_negative, group = topic, color = topic))+
  geom_line()+
  ggtitle("Neg Word Scores over time, by topic") +
  ylim(0,.04) +
  facet_wrap(~topic)

ggplot(topic_words, aes(Year, mean_negative, group = topic, color = topic))+
  geom_line()+
  ggtitle("Neg Word Scores over time,overall") +
  ylim(0,.04) 
  
ggplot(topic_words, aes(Year, mean_positive, group = topic, color = topic))+
  geom_line()+
  ggtitle("Positive Word Scores over time, by topic")+
  facet_wrap(~topic) 

ggplot(topic_words, aes(Year, mean_uncertainty, group = topic, color = topic))+
  geom_line()+
  ggtitle("Uncertainty Word Scores over time, by topic")+
  facet_wrap(~topic) +
  ylim(0, .035)


ggplot(topic_words, aes(Year, mean_weakmod, group = topic, color = topic))+
  geom_line()+
  ggtitle("Weakmod Word Scores over time, by topic")+
  facet_wrap(~topic) +
  ylim(0, .013)

ggplot(topic_words, aes(Year, mean_constraint, group = topic, color = topic))+
  geom_line()+
  ggtitle("Constraint Scores over time, by topic")+
  facet_wrap(~topic) 

ggplot(topic_words, aes(Year, mean_concrete, group = topic, color = topic))+
  geom_line()+
  ggtitle("Concrete Scores over time, by topic")+
  facet_wrap(~topic) +
  ylim(2.5, 3.1)

ggplot(topic_words, aes(Year, mean_future, group = topic, color = topic))+
  geom_line()+
  ggtitle("Future Scores over time, by topic")+
  facet_wrap(~topic) 

```


### STM: Number words (MEAN SPLIT SCORES)
```{r numberssub}

#merge in numbers
numbers_topic<- numbers %>%
  left_join(topic_df)

#wide to long
numbers_topics_long <- numbers_topic %>%
  gather(key = topic, value = value, t0.mean.d: t4.mean.d)%>%
  filter(value ==1)

#means
number_topics_means<- numbers_topics_long %>%
  group_by(Year, topic) %>%
  summarize(mean_per_numbers = mean(percent_numbers, na.rm=T))

#plot
number_topics_means%>%
  ggplot(aes(Year, mean_per_numbers, group = topic))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~topic)+
  ylim(0, 0.075)+
  ggtitle("Number words over time")
```