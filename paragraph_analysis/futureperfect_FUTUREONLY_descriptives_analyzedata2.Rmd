---
title: "Data Descriptives, Analyze Data (Part 2): THIS IS FOR FUTURE-ONLY"
author: "Carly Knight"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(stringr)
library(readxl)
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

data_loc = "~/Documents/Data/Annual Report/report_paragraphs/future_texts_paragraphs_futureonly/"
knitr::opts_knit$set(root.dir = normalizePath(data_loc))
```

# Basics Descriptives: Document Level
```{r basic}
load("text_data_prep_futureonly.R")

summary<-data_text %>%
  group_by(Year)%>%
  summarize(mean_numwords = mean(numwords_r, na.rm=T), 
            N = n())
summary %>%
  ggplot(aes(x=Year, y = mean_numwords)) +
  geom_line()+
  ggtitle("Number of words over time")

summary %>%
  ggplot(aes(x=Year, y = N)) +
  geom_line()+
  ggtitle("Number of docs over time")
```

# Basics Descriptives: Future-Words
```{r basic1}
#group by decade, future words
v <- c(  'may', 'might', 'future', 'will','optimism', 'pessimism', 'uncertainty', 'certainty',  'outlook', 'risk', 'risky', 'optimistic')

decade_words<- tbl_tokens%>%
  group_by(decade)%>%
  count(word, sort = TRUE) 

decade_totals <- decade_words %>%
  group_by(decade)%>%
  summarize(total= sum(n))

decade_words<- left_join(decade_words, decade_totals)
  
decade_words%>%
  filter(word %in% v) %>%
  mutate(percent = n/total) %>%
  ggplot(aes(decade, percent))+
  geom_line()+
  facet_wrap(~word, scales = "free")+
  ggtitle("Future words in text")

#tf-idf
decade_words_idf <- decade_words %>%
  bind_tf_idf(word, decade, n)

decade_words_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(decade) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = as.factor(decade))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~decade, ncol = 4, scales = "free") +
  coord_flip()+
  ggtitle("TF-IDF By Decade")

```


# Dictionary words
```{r dictionary, echo =FALSE}
#Concreteness ratings
#http://crr.ugent.be/archives/1330
concrete <- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/Concreteness_ratings_Brysbaert_et_al_BRM.csv")

concrete<-concrete %>%
  mutate(word = tolower(Word))

#Positive/Negative Snetiment
neg_emo <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Negative",col_names = "word")
constraint <- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Constraining",col_names = "word")
pos_emo<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "Positive",col_names = "word")
uncertainty<- read.csv(file = "~/Dropbox/PROJECTS/ConceptionsofRisk/data/dictionaries/uncertainty_businss_loughran_mcdonald.csv")
weak_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "WeakModal",col_names = "word")
strong_modal<- read_excel("~/Documents/Data/dictionaries/LoughranMcDonald_SentimentWordLists_2018.xlsx", sheet = "StrongModal",col_names = "word")

#concrete
concrete <- concrete %>%select(word, Conc.M)
  
#future
future_words <-read.csv(file = "/Users/carlyknight/Documents/Data/dictionaries/liwc2015dict.csv")   
future_words<- as_tibble(future_words)%>%
  filter(var == "FocusFuture")%>%
  mutate(word = gsub("\\*", "", word))%>%
  select(-c(var))

#lowercase dictionaries
neg_emo<- neg_emo %>% mutate(word = tolower(word)) %>% mutate(negsent = 1)
pos_emo<- pos_emo %>% mutate(word = tolower(word)) %>% mutate(possent = 1)
strong_modal<- strong_modal %>% mutate(word = tolower(word)) %>% mutate(strongmod = 1)
weak_modal<- weak_modal %>% mutate(word = tolower(word)) %>% mutate(weakmod = 1)
uncertainty<- uncertainty %>% mutate(word = tolower(word)) %>% mutate(uncertainty = 1)
constraint<- constraint %>% mutate(word = tolower(word)) %>% mutate(constraint = 1)
futurewords<- future_words %>% mutate(word = tolower(word)) %>% mutate(future = 1)
concrete<- concrete %>% mutate(word = tolower(word)) %>% mutate(concrete = Conc.M) %>% select(word, concrete)

#turn into a matrix
dic_matrix <- full_join(neg_emo, pos_emo)
dic_matrix <- full_join(dic_matrix, strong_modal)
dic_matrix <- full_join(dic_matrix, weak_modal)
dic_matrix <- full_join(dic_matrix, uncertainty)
dic_matrix <- full_join(dic_matrix, constraint)
dic_matrix <- full_join(dic_matrix, futurewords)
dic_matrix <- full_join(dic_matrix, concrete)


#join
tbl_tokens1 <- left_join(tbl_tokens, dic_matrix)

tbl_tokens1 <- tbl_tokens1 %>%
  select(Filename, paragraph_filename, Year, NAIC_broad, quantile_rank, word, negsent:concrete)%>%
  mutate(possent = ifelse(is.na(possent), 0, possent)) %>%
  mutate(negsent = ifelse(is.na(negsent), 0, negsent)) %>%
  mutate(uncertainty = ifelse(is.na(uncertainty), 0, uncertainty)) %>%
  mutate(constraint = ifelse(is.na(constraint), 0, constraint)) %>%
  mutate(strongmod = ifelse(is.na(strongmod), 0, strongmod)) %>%
  mutate(future = ifelse(is.na(future), 0, future)) %>%
  mutate(weakmod = ifelse(is.na(weakmod), 0, weakmod))

rm(tbl_tokens)
gc()
#save(tbl_tokens1, file = "/Users/carlyknight/Dropbox/PROJECTS/ConceptionsofRisk/data/dtm/tbl_tokens1.R" )
#format(object.size(tbl_tokens1), units = "Gb") #11.4Gb

#year
year_words<- tbl_tokens1%>%
  group_by(Year)%>%
  summarize(mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T),
            mean_constraint = mean(constraint, na.rm =T),
            mean_weakmod = mean(weakmod, na.rm=T),
            mean_strongmod = mean(strongmod, na.rm =T),
            mean_future = mean(future, na.rm =T),
            mean_concrete = mean(concrete, na.rm=T),
            N = n())

year_words%>%
  gather(key = varname, value = value, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time 1 ")

year_words%>%
  gather(key = varname, value = value, mean_constraint, mean_weakmod, mean_strongmod)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  facet_wrap(~varname, ncol =2)+
  ggtitle("Word Scores over time 2")

year_words%>%
  gather(key = varname, value = value, mean_concrete)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_point()+
  xlim(1930, 2005)+
  ylim(2, 3.5)+
  ggtitle("Concrete over time") 

year_words%>%
  gather(key = varname, value = value, mean_future)%>%
  ggplot(aes(Year, value, color = varname))+
  geom_line()+
  xlim(1930, 2005)+
  ylim(0, .02)+
  ggtitle("Future Words over time") 
```

### Dictionary Words, by Subgroup
```{r dictionarysub, echo =FALSE}
#by industry
tbl_tokens1%>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
            mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n()) %>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive,  mean_uncertainty)%>%
  ggplot(aes(Year, value, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, industry")

tbl_tokens1%>%
  group_by(Year, NAIC_broad)%>%
  filter(!is.na(NAIC_broad)) %>%
  summarize(mean_concrete = mean(concrete, na.rm =T)) %>%
  ggplot(aes(Year, mean_concrete, group = NAIC_broad, color = NAIC_broad))+
  geom_line()+
  ylim(1.5, 3.5)+
  ggtitle("Concrete Scores over time, industry")

tbl_tokens1%>%
  group_by(Year, quantile_rank)%>%
  summarize(mean_constraint = mean(constraint, na.rm =T),
            mean_positive = mean(possent, na.rm =T),
            mean_negative = mean(negsent, na.rm =T),
              mean_uncertainty = mean(uncertainty, na.rm =T), 
            N = n())%>%
  filter(quantile_rank ==1 | quantile_rank ==4)%>%
  gather(key = varname, value = value, mean_constraint, mean_negative, mean_positive, mean_uncertainty)%>%
  ggplot(aes(Year, value, group = as.factor(quantile_rank), color = as.factor(quantile_rank)))+
  geom_point()+
  facet_wrap(~varname, scales= "free", ncol =2)+
  ggtitle("Word Scores over time, size")

```

# Words changing over time
```{r models, echo=FALSE}
#total number words
nwords<-  length(unique(tbl_tokens1$word))

graph_word_change<- function(wordvar){

#calculate word frequency
words_by_time <- tbl_tokens1 %>%
  filter(get(wordvar) ==1)%>%
  count(Year, word)%>%
  group_by(Year) %>%
  mutate(words_year_total = sum(n)) %>%
  group_by(word)%>%
  mutate(words_dataset_total = sum(n))%>%
  group_by(Year, word) %>%
  mutate(word_total = sum(n)) %>%
  mutate(word_freq = words_dataset_total/nwords)%>%
  mutate(word_percent =word_total/words_year_total )%>%
  ungroup() %>%
  filter(word_total > 30)%>%
  select(Year, word,word_percent, word_freq)
 
models1 <- words_by_time %>%
  group_by(word) %>%
  do(tidy(lm( word_percent~ Year, .,))) %>%
  ungroup() %>%
  filter(term =="Year")%>%
  mutate(adjustedp = p.adjust(p.value))

freqs<- unique(words_by_time %>% 
                 select(word, word_freq))

top.slopes<- models1 %>% 
  filter(adjustedp <.05)

top.slopes%>%
  inner_join(freqs) %>%
  ggplot(aes(estimate, word_freq)) +
  geom_point(size = 0.5) +
  scale_y_log10() +
  geom_text(aes(label = word), check_overlap = TRUE)+
  xlab("Estimated change over time") +
  ylab("Frequency, logged scale")+
  geom_vline(xintercept=0, color="red")+
  theme_minimal()+
  ggtitle(paste(wordvar, ": word change over time", sep = " "))
}

#Graph
word_types<- c("possent", "negsent", "uncertainty", "constraint", "weakmod", "future")


graph_word_change("possent")
graph_word_change("negsent")
graph_word_change("constraint")
graph_word_change("future")
graph_word_change("uncertainty")
graph_word_change("weakmod")
graph_word_change("strongmod")
```

# Readability 
```{r readability, echo = FALSE}
library(quanteda)

#create corpus
# corpus <- corpus(data_full, docid_field = "filename", text_field = "text")
# 
# #readability
# read<-textstat_readability(corpus, measure = c("Flesch", "FOG", "meanSentenceLength", "meanWordSyllables"), remove_hyphens = TRUE, min_sentence_length = 1, max_sentence_length = 10000)
# 
# #merge with dates
# read2<-read %>%
#   rename(filename = document)%>%
#   left_join(data)%>%
#   group_by(Year)%>%
#   summarize(mean_fog = mean(FOG, na.rm =T),
#             mean_Flesch = mean(Flesch, na.rm =T),
#             mean_sentencelength = mean(meanSentenceLength, na.rm =T),
#             mean_syllables = mean(meanWordSyllables, na.rm =T),
#             N = n())
# 
# read2%>%
#   gather(key = varname, value = value, mean_fog, mean_Flesch, mean_sentencelength, mean_syllables)%>%
#   ggplot(aes(Year, value, color = varname))+
#   geom_point()+
#   xlim(1930, 2005)+
#   ggtitle("Readability score over time")+
#   theme_minimal()
# ggsave("readability.pdf")

```

#Number words
```{r numbers, echo=FALSE}
#generate graphs
number_means<- numbers %>%
  group_by(Year) %>%
  summarize(mean_per_numbers = mean(percent_numbers, na.rm=T))

number_means%>%
  ggplot(aes(Year, mean_per_numbers))+
  geom_point()+
  ylim(0, .07)+
  xlim(1930, 2005)+
  ggtitle("Number words over time")
```

